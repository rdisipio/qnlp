{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import string\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "from torch.optim import SGD\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import pennylane as qml\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CBOW(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_size, context_size, hidden_size=512):\n",
    "        super(CBOW, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_size = embedding_size\n",
    "        self.context_size = context_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embeddings = nn.Embedding(self.vocab_size, self.embedding_size)\n",
    "        # return vector size will be context_size*2*embedding_size\n",
    "        self.lin1 = nn.Linear(self.context_size * 2 * self.embedding_size, self.hidden_size)\n",
    "        self.lin2 = nn.Linear(self.hidden_size, self.vocab_size)\n",
    "    \n",
    "    def forward(self, inp):\n",
    "        out = self.embeddings(inp).view(1, -1)\n",
    "        out = out.view(1, -1)\n",
    "        out = self.lin1(out)\n",
    "        out = F.relu(out)\n",
    "        out = self.lin2(out)\n",
    "        out = F.log_softmax(out, dim=1)\n",
    "        return out\n",
    "    \n",
    "    def get_word_vector(self, word_idx):\n",
    "        word = Variable(torch.LongTensor([word_idx]))\n",
    "        return self.embeddings(word).view(1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_qubits = 2\n",
    "dev = qml.device(\"default.qubit\", wires=n_qubits)\n",
    "\n",
    "@qml.qnode(dev)\n",
    "def qnode(inputs, weights):\n",
    "    qml.templates.AngleEmbedding(inputs, wires=range(n_qubits))\n",
    "    qml.templates.BasicEntanglerLayers(weights, wires=range(n_qubits))\n",
    "    return [qml.expval(qml.PauliZ(wires=i)) for i in range(n_qubits)]\n",
    "\n",
    "class QCBOW(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_size, context_size, hidden_size=512, n_layers=2, n_qubits=4):\n",
    "        super(QCBOW, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_size = embedding_size\n",
    "        self.context_size = context_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embeddings = nn.Embedding(self.vocab_size, self.embedding_size)\n",
    "        self.weight_shapes = {\"weights\": (n_layers, n_qubits)}\n",
    "        qlayer = qml.qnn.TorchLayer(qnode, weight_shapes)\n",
    "\n",
    "    def forward(self, inp):\n",
    "        out = self.embeddings(inp).view(1, -1)\n",
    "        out = out.view(1, -1)\n",
    "        out = self.lin1(out)\n",
    "        out = F.relu(out)\n",
    "        out = self.lin2(out)\n",
    "        out = F.log_softmax(out, dim=1)\n",
    "        return out\n",
    "    \n",
    "    def get_word_vector(self, word_idx):\n",
    "        word = Variable(torch.LongTensor([word_idx]))\n",
    "        return self.embeddings(word).view(1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_cbow(cbow, data, unique_vocab, word_to_idx, n_epochs=20):\n",
    "    \n",
    "    nll_loss = nn.NLLLoss()  # loss function\n",
    "    optimizer = SGD(cbow.parameters(), lr=0.001)\n",
    "    \n",
    "    print(\"Starting model training...\")\n",
    "    \n",
    "    history = {\n",
    "        'loss': []\n",
    "    }\n",
    "    for iepoch in range(n_epochs):\n",
    "        total_loss = 0\n",
    "        for context, target in data:            \n",
    "            inp_var = Variable(torch.LongTensor([word_to_idx[word] for word in context]))\n",
    "            target_var = Variable(torch.LongTensor([word_to_idx[target]]))\n",
    "                        \n",
    "            cbow.zero_grad()\n",
    "            log_prob = cbow(inp_var)\n",
    "            loss = nll_loss(log_prob, target_var)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.data\n",
    "        \n",
    "        loss_avg = float(total_loss / len(data))\n",
    "        print(\"{}/{} loss {:.2f}\".format(iepoch, n_epochs, loss_avg))\n",
    "        history['loss'].append(loss_avg)\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_cbow(cbow, unique_vocab, word_to_idx):\n",
    "    # test word similarity\n",
    "    word_1 = unique_vocab[2]\n",
    "    word_2 = unique_vocab[3]\n",
    "    \n",
    "    word_1_vec = cbow.get_word_vector(word_to_idx[word_1])[0]\n",
    "    word_2_vec = cbow.get_word_vector(word_to_idx[word_2])[0]\n",
    "    \n",
    "    word_similarity = (word_1_vec.dot(word_2_vec) / (torch.norm(word_1_vec) * torch.norm(word_2_vec))).data.numpy()\n",
    "    print(\"Similarity between '{}' & '{}' : {:0.4f}\".format(word_1, word_2, word_similarity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_text = \"This tutorial will walk you through the key ideas of deep learning programming using Pytorch.\" \\\n",
    "              \" Many of the concepts (such as the computation graph abstraction and autograd) \" \\\n",
    "              \"are not unique to Pytorch and are relevant to any deep learning tool kit out there.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(corpus_text, context_size, embed_dim, n_epochs=20):\n",
    "    # consider 2*context_size as context window where middle word as target\n",
    "    corpus_text = corpus_text.split(' ')\n",
    "    for i in range(len(corpus_text)):\n",
    "        sentence = corpus_text[i]\n",
    "        cleaned_sentence = sentence.translate(str.maketrans('', '', string.punctuation))\n",
    "        cleaned_sentence = cleaned_sentence.lower()\n",
    "        corpus_text[i] = cleaned_sentence\n",
    "    unique_vocab = list(set(corpus_text))\n",
    "    unique_vocab = sorted(unique_vocab)\n",
    "    vocab_size = len(unique_vocab)\n",
    "    print(f\"There are {vocab_size} unique words in the vocabulary\")\n",
    "    # mapping to index\n",
    "    word_to_idx = {w: i for i, w in enumerate(unique_vocab)}\n",
    "    print(word_to_idx)\n",
    "\n",
    "    data = list()\n",
    "    for i in range(context_size, len(corpus_text) - context_size):\n",
    "        data_context = list()\n",
    "        for j in range(context_size):\n",
    "            data_context.append(corpus_text[i - context_size + j])\n",
    "        \n",
    "        for j in range(1, context_size + 1):\n",
    "            data_context.append(corpus_text[i + j])\n",
    "        data_target = corpus_text[i]\n",
    "        data.append((data_context, data_target))\n",
    " \n",
    "    print(\"Some data: \",data[:3])\n",
    "\n",
    "    train_data, test_data = train_test_split(data, test_size=0.3)\n",
    "    print(f\"Training set size: {len(train_data)}\")\n",
    "    print(f\"Test set size: {len(test_data)}\")\n",
    "    # define and train model\n",
    "    #cbow = CBOW(vocab_size, embed_dim, context_size)\n",
    "    cbow = QCBOW(vocab_size, embed_dim, context_size)\n",
    "    history = train_cbow(cbow, train_data, unique_vocab, word_to_idx, n_epochs)\n",
    "    \n",
    "    # get two words similarity\n",
    "    test_cbow(cbow, unique_vocab, word_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 34 unique words in the vocabulary\n",
      "{'abstraction': 0, 'and': 1, 'any': 2, 'are': 3, 'as': 4, 'autograd': 5, 'computation': 6, 'concepts': 7, 'deep': 8, 'graph': 9, 'ideas': 10, 'key': 11, 'kit': 12, 'learning': 13, 'many': 14, 'not': 15, 'of': 16, 'out': 17, 'programming': 18, 'pytorch': 19, 'relevant': 20, 'such': 21, 'the': 22, 'there': 23, 'this': 24, 'through': 25, 'to': 26, 'tool': 27, 'tutorial': 28, 'unique': 29, 'using': 30, 'walk': 31, 'will': 32, 'you': 33}\n",
      "Some data:  [(['this', 'tutorial', 'will', 'you', 'through', 'the'], 'walk'), (['tutorial', 'will', 'walk', 'through', 'the', 'key'], 'you'), (['will', 'walk', 'you', 'the', 'key', 'ideas'], 'through')]\n",
      "Training set size: 25\n",
      "Test set size: 12\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'weight_shapes' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-267a7890495a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0membed_dim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m128\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mn_epochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membed_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-7-ba74fa39978a>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(corpus_text, context_size, embed_dim, n_epochs)\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;31m# define and train model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;31m#cbow = CBOW(vocab_size, embed_dim, context_size)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m     \u001b[0mcbow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mQCBOW\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membed_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m     \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_cbow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcbow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munique_vocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_to_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-308f8f09cfe1>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, vocab_size, embedding_size, context_size, hidden_size, n_layers, n_qubits)\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEmbedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight_shapes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"weights\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mn_layers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_qubits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mqlayer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mqml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTorchLayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqnode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_shapes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'weight_shapes' is not defined"
     ]
    }
   ],
   "source": [
    "context_size = 3\n",
    "embed_dim = 128\n",
    "n_epochs = 20\n",
    "main(corpus_text, context_size, embed_dim, n_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qml",
   "language": "python",
   "name": "qml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
